{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Set 5, Problem 6\n",
    "\n",
    "In this exercise, we are going to deal with stochastic differential equation and diffusion model with non-Euclidean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions import Distribution, Categorical\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.transform import Rotation\n",
    "\n",
    "torch.manual_seed(2024);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### It么 process\n",
    "Consider the stochastic differential equation (SDE) $dX_t = f(X_t, t)dt + g(X_t, t)dW_t$. \n",
    "\n",
    "$f(X_t, t)dt$ is the drift term, which reflects the smooth, predictable contribution to dx, akin to the deterministic rate of change seen in ordinary differential equations.\n",
    "\n",
    "$g(X_t, t)dW_t$ is the diffusion term, which adds randomness to the change $dx$, modeled as the product of a volatility factor $g(X_t, t)$ and the increment $dW_t$ of a Wiener process.\n",
    "\n",
    "When the function of X is given as $\\psi (X)$, we can derive $d\\psi$ as follows by applying the It么 rule.\n",
    "\n",
    "$d\\psi = \\frac{\\partial \\psi}{\\partial X} (f dt + g dW) + \\frac{1}{2} \\frac{\\partial^2 \\psi}{\\partial X^2} g^2 dt$\n",
    "\n",
    "We are going to show that the second derivative term is crucial to describe the change of $\\psi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define the function **forward_process**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psi_function(x):\n",
    "    '''\n",
    "    < input >\n",
    "    x: torch.Tensor (b, 2)\n",
    "    < output >\n",
    "    psi: torch.Tensor (b, )\n",
    "    '''\n",
    "    return (x[:, 0]**2 + x[:, 1]**2)\n",
    "\n",
    "def drift_term(x, t):\n",
    "    R = torch.Tensor([[0, -1], [1, 0]])\n",
    "    return torch.mm(x, R.t())\n",
    "\n",
    "def diffusion_term(x, t):\n",
    "    return torch.ones_like(x) * 0.1\n",
    "\n",
    "def forward_process(x, t, f, g, dt = 0.01):\n",
    "    \"\"\"\n",
    "    Forward process for a stochastic differential equation\n",
    "    dx = f(x, t) dt + g(x, t) dW\n",
    "    < input >\n",
    "    x: torch.Tensor (b, 2)\n",
    "    t: float\n",
    "    f, g: functions\n",
    "    < output >\n",
    "    x_next: torch.Tensor (b, 2)\n",
    "    \"\"\"\n",
    "    ############### YOUR CODE HERE ###############\n",
    "    x_next = None\n",
    "    ##############################################\n",
    "    return x_next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we need to define the function **compute_gradient** and **compute_hessian**. Applying these two functions, we can find $d\\psi$ with It么 rule. \n",
    "\n",
    "Using torch.autograd.grad() might be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(psi, x):\n",
    "    \"\"\"\n",
    "    Compute the gradient of psi with respect to x\n",
    "    < input >\n",
    "    psi: function of x\n",
    "    x: torch.Tensor (b, 2)\n",
    "    t: float\n",
    "    < output >\n",
    "    dpsi_dx: torch.Tensor (b, 2)\n",
    "    \"\"\"\n",
    "    ############### YOUR CODE HERE ###############\n",
    "    dpsi_dx = None\n",
    "    ##############################################\n",
    "    return dpsi_dx\n",
    "\n",
    "def compute_hessian(psi, x):\n",
    "    \"\"\"\n",
    "    Compute the hessian of psi with respect to x\n",
    "    < input >\n",
    "    psi: function of x\n",
    "    x: torch.Tensor (b, 2)\n",
    "    t: float\n",
    "    < output >\n",
    "    d2psi_dx2: torch.Tensor (b, 2, 2)\n",
    "    \"\"\"\n",
    "    ############### YOUR CODE HERE ###############\n",
    "    d2psi_dx2 = None\n",
    "    ##############################################\n",
    "    return d2psi_dx2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the return value of the functions are in the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 2)\n",
    "x.requires_grad = True\n",
    "psi = psi_function(x)\n",
    "assert psi.shape == (x.shape[0], )\n",
    "assert compute_gradient(psi_function, x).shape == x.shape\n",
    "assert compute_hessian(psi_function, x).shape == (x.shape[0], x.shape[1], x.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we derive $d\\phi$ applying It么 rule. For the second function, do not include the second derivative term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_psi_with_second_derivative(psi, x, t, f, g, dt = 0.01):\n",
    "    \"\"\"\n",
    "    Ito process for a stochastic differential equation. This function includes the second derivative of psi.\n",
    "    < input >\n",
    "    psi: function of x\n",
    "    x: torch.Tensor (b, 2)\n",
    "    t: float\n",
    "    f, g: functions\n",
    "    < output >\n",
    "    dpsi: torch.Tensor (b, )\n",
    "    \"\"\"\n",
    "    ############### YOUR CODE HERE ###############\n",
    "    dpsi = None\n",
    "    ##############################################\n",
    "    return dpsi\n",
    "\n",
    "def d_psi_without_second_derivative(psi, x, t, f, g, dt = 0.01):\n",
    "    \"\"\"\n",
    "    Ito process for a stochastic differential equation. This function does not include the second derivative of psi.\n",
    "    < input >\n",
    "    psi: function of x\n",
    "    x: torch.Tensor (b, 2)\n",
    "    t: float\n",
    "    f, g: functions\n",
    "    < output >\n",
    "    dpsi: torch.Tensor (b, )\n",
    "    \"\"\"\n",
    "    ############### YOUR CODE HERE ###############\n",
    "    dpsi = None\n",
    "    ##############################################\n",
    "    return dpsi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the initial distribution, we use a mixture of Gaussian distributions with means at $(0.5, 0)$ and $(-0.5, 0)$, and a common variance of $0.01 \\cdot I_{2\\times 2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianMixture(Distribution):\n",
    "    def __init__(self, means, covariances):\n",
    "        self.means = means\n",
    "        self.covariances = covariances\n",
    "        self.weights = torch.tensor([0.5, 0.5])\n",
    "        self.categorical = Categorical(self.weights)\n",
    "    def sample(self, sample_shape = torch.Size()):\n",
    "        component_samples = self.categorical.sample(sample_shape)\n",
    "        samples = []\n",
    "        for i in range(2):\n",
    "            gaussian = MultivariateNormal(self.means[i], self.covariances[i])\n",
    "            component_sample = gaussian.sample(sample_shape)\n",
    "            samples.append(component_sample[component_samples == i])\n",
    "        return torch.cat(samples, dim=0)\n",
    "    def log_prob(self, value):\n",
    "        log_probs = torch.stack([MultivariateNormal(self.means[i], covariance_matrix=self.covariances[i]).log_prob(value)\n",
    "                                 for i in range(2)])\n",
    "        weighted_log_probs = torch.logsumexp(torch.log(self.weights) + log_probs, dim=0)\n",
    "        return weighted_log_probs\n",
    "    \n",
    "means = torch.tensor([[0.5, 0], [-0.5, 0]])\n",
    "covariances = torch.stack([torch.eye(2) * 0.01, torch.eye(2) * 0.01])\n",
    "dist = GaussianMixture(means, covariances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample 100 points to visualize the distribution of the mixture of Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dist.sample((100, ))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "ax.axis('equal')\n",
    "ax.scatter(x[:, 0], x[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the $d\\psi$ functions we wrote, let's predict how the $\\psi$ function changes according to the changes in $X$. \n",
    "Then, compare how similar the distribution of $\\psi(X)$ after the change is to the predicted distribution. \n",
    "We will use Wasserstein distance to measure the distance between the two distributions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 100\n",
    "dt = 0.01\n",
    "t = torch.arange(0, 1.57, dt)\n",
    "x = dist.sample((batch, ))\n",
    "x.requires_grad = True\n",
    "\n",
    "psi1 = psi_function(x)\n",
    "psi2 = psi_function(x)\n",
    "\n",
    "x_list = [x]\n",
    "psi1_list = [psi1]\n",
    "psi2_list = [psi2]\n",
    "for i in range(len(t)):\n",
    "    x = forward_process(x, t[i], drift_term, diffusion_term, dt)\n",
    "    dpsi1 = d_psi_with_second_derivative(psi_function, x, t[i], drift_term, diffusion_term, dt)\n",
    "    dpsi2 = d_psi_without_second_derivative(psi_function, x, t[i], drift_term, diffusion_term, dt)\n",
    "    psi1 = psi1 + dpsi1\n",
    "    psi2 = psi2 + dpsi2\n",
    "    x_list.append(x)\n",
    "    psi1_list.append(psi1)\n",
    "    psi2_list.append(psi2)\n",
    "\n",
    "psi1 = psi1_list[-1]\n",
    "psi2 = psi2_list[-1]\n",
    "psi_true = psi_function(x_list[-1])\n",
    "\n",
    "distance1 = wasserstein_distance(psi1.detach().numpy(), psi_true.detach().numpy())\n",
    "distance2 = wasserstein_distance(psi2.detach().numpy(), psi_true.detach().numpy())\n",
    "print(\"Wasserstein distance\")\n",
    "print(f\"It么 rule with second derivative : {distance1}\")\n",
    "print(f\"It么 rule without second derivative : {distance2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Explain the above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. ~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Diffusion model on SO(3)\n",
    "\n",
    "This time, we will explore diffusion models based on SDEs. The content will specifically focus on score-based generative modeling, where the drift term is zero in the forward process.\n",
    "We recommend referring to the paper \"Generative Modeling by Estimating Gradients of the Data Distribution\" by Yang Song et al. for further details.\n",
    "\n",
    "Especially we are going to use Denoising Score Matching and sampling with annealed Langevin dynamics. It perturbs the data point X with a pre-specified noise distribution $q_{\\sigma}(\\tilde{X}|X)$ and then employs score matching to estimate the score of the perturbed data distribution $q_{\\sigma}(\\tilde{X}) \\triangleq \\int q_{\\sigma} (\\tilde{X}|X)p_{data}(X)dX$.\n",
    "\n",
    "We use $q_\\sigma (\\tilde{X}|X) = \\mathcal{N}(\\tilde{X}|X, \\sigma^2 I)$ as the noise distribution resulting in $\\nabla_{\\tilde{X}} \\log q_\\sigma (\\tilde{X}|X) = - \\frac{\\tilde{X}-X}{\\sigma^2}$. \n",
    "Consequently, the denosing score matching objective for the scheduled $\\sigma_t$ can be expressed as follows.\n",
    "\n",
    "$\\mathcal{L}(\\theta; \\sigma) \\triangleq \\frac{1}{2} \\mathbb{E}_{t \\sim \\mathcal{U}(0, T)} \\mathbb{E}_{X \\sim p_{data(X)}} \\mathbb{E}_{\\tilde{X} \\sim \\mathcal{N}(X, \\sigma_t^2 I)} || s_\\theta (\\tilde{X}, \\sigma_t) + \\frac{\\tilde{X}-X}{\\sigma_t^2}||_2^2$\n",
    "\n",
    "For sampling, we employ annealed Langevin dynamics, and the data distribution can be approached using the following equation for the scheduled $\\alpha_i$.\n",
    "\n",
    "$X_t \\leftarrow X_{t+1} + \\frac{\\alpha_i}{2} s_\\theta (X_{t+1}, \\sigma_i) + \\sqrt{\\alpha_i} z_t$, where $z_t \\sim \\mathcal{N}(0, I)$, $t = T-1, T-2, \\cdots, 1, 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the above operation to the SO(3) manifold, perturbations are defined on the tangent plane, specifically in the corresponding $\\mathbb{R}^3$ space, and the data points are moved using the matrix exponential. \n",
    "Applying noise with increasing variance to the data distribution brings it closer to a uniform distribution on the SO(3) manifold. \n",
    "Consequently, starting from this uniform distribution, the data distribution can be recovered by applying scores learned through training the model.\n",
    "For more details, refer to the paper \"SE(3)-DiffusionFields: Learning smooth cost functions for joint grasp and motion optimization through diffusion\" by Julen Urain et al."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.Lie import Logmap, Expmap\n",
    "\n",
    "def sample_from_data(batch: int):\n",
    "    '''\n",
    "    This function generates a batch of rotation matrices, which describes the rotation with respect to the z-axis.\n",
    "    '''\n",
    "    theta = torch.rand(batch) * 2 * np.pi\n",
    "    return Expmap(torch.stack([torch.zeros_like(theta), torch.zeros_like(theta), theta], dim=1))\n",
    "\n",
    "def marginal_prob_std(t, sigma = 0.5):\n",
    "    '''\n",
    "    It computes the scheduled standard deviation at time t. (sigma_t)\n",
    "    '''\n",
    "    return 2 * np.sqrt((sigma ** (2 * t) - 1.) / (2. * np.log(sigma)))\n",
    "\n",
    "class diffusion_SO3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(9+1, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 512), \n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 3)\n",
    "        )\n",
    "        self.timesteps = torch.arange(0, 1, 0.005)\n",
    "\n",
    "    def __call__(self, R, t):\n",
    "        return self.score_function(R, t)\n",
    "    \n",
    "    def init_sample(self, batch: int):\n",
    "        return torch.tensor(Rotation.random(batch).as_matrix(), dtype=torch.float32)\n",
    "    \n",
    "    def sample(self, batch: int):\n",
    "        steps_fit = 50\n",
    "        R = self.init_sample(batch)\n",
    "        for t in self.timesteps: # annealed Langevin dynamics\n",
    "            R = self._step(R, 1-t, noise_off = False)\n",
    "        for _ in range(steps_fit): # extra steps for fitting\n",
    "            R = self._step(R, 0, noise_off = True)\n",
    "        return R\n",
    "    \n",
    "    def _step(self, R, t, noise_off = False):\n",
    "        '''\n",
    "        This function is used for sampling. \n",
    "        From the current rotation matrix R and time t, it generates the next rotation matrix R_next.\n",
    "        '''\n",
    "        batch = R.shape[0]\n",
    "        eps = 1e-3\n",
    "        time = t * (1-eps) + eps\n",
    "        sigma_T = marginal_prob_std(eps)\n",
    "        sigma_i = marginal_prob_std(time)\n",
    "        ratio = sigma_i ** 2 / sigma_T ** 2\n",
    "        alpha = 1e-3 * ratio\n",
    "        noise = torch.randn(batch, 3) * 0.5\n",
    "        if noise_off:\n",
    "            alpha = 0.003\n",
    "            noise = torch.zeros(batch, 3)\n",
    "        '''\n",
    "        R: torch.Tensor (b, 3, 3)\n",
    "        time: float\n",
    "        noise: torch.Tensor (b, 3)\n",
    "        alpha: float\n",
    "        < output > (Hint: use score_function and Expmap)\n",
    "        R_next: torch.Tensor (b, 3, 3)\n",
    "        '''\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        R_next = None\n",
    "        ##############################################\n",
    "        return R_next\n",
    "    \n",
    "    def score_function(self, R, t):\n",
    "        '''\n",
    "        It computes the score function of the diffusion process using neural network.\n",
    "        < input >\n",
    "        R: torch.Tensor (b, 3, 3)\n",
    "        t: torch.Tensor (b, )\n",
    "        < output > (Hint: use self.net)\n",
    "        score: torch.Tensor (b, 3)\n",
    "        '''\n",
    "        ############### YOUR CODE HERE ###############\n",
    "        score = None\n",
    "        ##############################################\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = diffusion_SO3()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "batch = 1000\n",
    "epoch = 10000\n",
    "loss_list = []\n",
    "for e in range(epoch):\n",
    "    t = torch.rand(batch) * (1 - 1e-3) + 1e-3\n",
    "    std = torch.tensor(marginal_prob_std(t.numpy()), dtype = torch.float32).unsqueeze(-1)\n",
    "    R_data = sample_from_data(batch)\n",
    "    noise = torch.randn(batch, 3)\n",
    "    R_perturb = torch.bmm(R_data, Expmap(noise * std))\n",
    "    score_predict = model(R_perturb, t)\n",
    "    score_target = - noise / std # == - (x_tilde - x) / std^2\n",
    "    \n",
    "    loss = ((score_predict - score_target).pow(2).sum(-1)).mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (e+1) % 100 == 0:\n",
    "        loss_list.append(loss.item())\n",
    "        if (e+1) % 1000 == 0:\n",
    "            print(f\"Epoch {e+1} : {loss.item()}\")\n",
    "    \n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code visually illustrates the initial and final distributions of the diffusion model. \n",
    "The result shows the columns of the rotation matrices tranformed into frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 100\n",
    "R0 = model.init_sample(batch).detach().numpy()\n",
    "R = model.sample(batch).detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_zlim(-1, 1)\n",
    "for r in R0:\n",
    "    ax.quiver(0, 0, 0, r[0, 0], r[1, 0], r[2, 0], color='tab:red')\n",
    "    ax.quiver(0, 0, 0, r[0, 1], r[1, 1], r[2, 1], color='tab:green')\n",
    "    ax.quiver(0, 0, 0, r[0, 2], r[1, 2], r[2, 2], color='tab:blue')\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_xlim(-1, 1)\n",
    "ax.set_ylim(-1, 1)\n",
    "ax.set_zlim(-1, 1)\n",
    "for r in R:\n",
    "    ax.quiver(0, 0, 0, r[0, 0], r[1, 0], r[2, 0], color='tab:red')\n",
    "    ax.quiver(0, 0, 0, r[0, 1], r[1, 1], r[2, 1], color='tab:green')\n",
    "    ax.quiver(0, 0, 0, r[0, 2], r[1, 2], r[2, 2], color='tab:blue')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q. Explain the above results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. ~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds2set",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
